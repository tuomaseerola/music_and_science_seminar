{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "audio_analysis_tutorial.ipynb",
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tuomaseerola/music_and_science_seminar/blob/master/seminar2A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTO_mMkGJCci"
   },
   "source": [
    "# Music and Science – Audio Analysis Tutorial \n",
    "\n",
    "[Tuomas Eerola](https://www.durham.ac.uk/staff/tuomas-eerola/), Durham University, Music Department, 2022.\n",
    "\n",
    "For **Music and Science** Module, Seminar 2\n",
    "\n",
    "_Last update 30/10/2022 by TE_"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 1: Preliminaries\n",
    "\n",
    "Welcome to Colab. This is a notebook that can run python code within a cloud service and offer the results within the same web page. You can edit any code blocks and try out what happens. Note that your changes will not have permanent impact on the notebook, as this notebook will be reset when you leave the page.\n",
    "\n",
    "We are going to go through the basics of analysing audio examples. There will be 5 _Learning Tasks_ to prompt you to try out the analysis your self and the tasks will become more difficult towards the end.\n",
    "\n",
    "To start the analysis, click **play button** on the top left corner of the code below. This will setup the system for this notebook. [Librosa](https://librosa.org/doc/0.8.1/) is the audio analysis tool (created by Brian McFee and others, see their [paper](https://conference.scipy.org/proceedings/scipy2015/pdfs/brian_mcfee.pdf)) that we will be using and the rest of the libraries that are activated here are for convenience and creating graphs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r3LSkxYNS5nY"
   },
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "print(librosa.__version__)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2SD2rhaHDF9"
   },
   "source": [
    "# Section 2: Read an audio file\n",
    "\n",
    "## Section 2.1: Read Librosa example excerpts\n",
    "\n",
    "Here are some example excerpts that come with Librosa. We can use any of them in the subsequent sections. Just remove the hashtag (#) in front of the line to get that audio file.\n",
    "\n",
    "The following code also shows how to select only a _segment_ of an audio file. This is done by keywords `offset` and `duration`. Offset specifies where you want to start the segment (in seconds) and duration (also in seconds) is self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "trn2ya5RNovU"
   },
   "source": [
    "#filename = librosa.ex('trumpet')   # Mihai Sorohan - Monophonic trumpet recording\n",
    "#filename = librosa.ex('brahms')    # Hungarian Dance no. 5\n",
    "#filename = librosa.ex('choice')    # A short drum and bass loop\n",
    "filename = librosa.ex('fishin')     # Karissa Hobbs - Let’s Go Fishin’ A folk/pop song with verse/chorus/verse structure and vocals\n",
    "#filename = librosa.ex('nutcracker')# Tchaikovsky - Dance of the Sugar Plum Fairy\n",
    "#filename = librosa.ex('vibeace')   # 60-second clip\n",
    "x, sr = librosa.load(filename, duration=20) # if you want to start the segment from a specific point, add offset = [2 for example] in the command\n",
    "\n",
    "plt.figure(figsize=(12, 4))                  # create figure\n",
    "librosa.display.waveshow(y = x,sr = sr)      # plot waveform\n",
    "ipd.display(ipd.Audio(data = x, rate = sr))  # create playback object"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### _Learning Task 1_\n",
    "---\n",
    "_Try out some of the audio examples yourself. To select one, remove the hashtag (`#`) from the beginning of the line. NB. Only one audio example should be run at a time, so make sure that the rest of the 'filename' commands have a # in front of them._"
   ],
   "metadata": {
    "id": "G2y2Sd86yV9i"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIxDVjKwaFfM"
   },
   "source": [
    "## Section 2.2: Load audio files from online sources\n",
    "This is how to load example sound files from Github pages (or anywhere from online, if you know the URL)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5cgjfvXZaMA3"
   },
   "source": [
    "import soundfile as sf\n",
    "import io\n",
    "from six.moves.urllib.request import urlopen\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/clar_one_note.wav\"\n",
    "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/trumpet_one_note.wav\"\n",
    "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/harp_one_note.wav\"\n",
    "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/harpsichord_one_note.wav\"\n",
    "\n",
    "x, sr = sf.read(io.BytesIO(urlopen(url).read()))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(y = x,sr = sr, max_sr = 100, color = 'purple')\n",
    "ipd.display(ipd.Audio(data = x, rate = sr))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 2.3: Load audio files from your computer / google drive (optional)\n",
    "It is also possible to load any audio you have on your hard drive to the workspace of the notebook by clicking the folder icon (on the left) and choosing the file, waiting for the file to upload, and then referring to the file. These audio files will **NOT be saved** across the sessions unless you connect your google drive with Colab, which is easy (see [External data: Local Files, Drive, Sheets and Cloud Storage](https://colab.research.google.com/notebooks/io.ipynb)).   "
   ],
   "metadata": {
    "id": "8ChDg4JYzpPb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Y, sr = librosa.load('my_own_recording.wav') # if you want to start the segment from a specific point, add offset = [2 for example] in the command\n",
    "plt.figure(figsize=(12, 4))\n",
    "ipd.display(ipd.Audio(data = Y, rate = sr))"
   ],
   "metadata": {
    "id": "iCk5C6KezLln"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwcceopAD2wz"
   },
   "source": [
    "# Section 3: Explore acoustic features across time\n",
    "First we extract RMS (Root Mean Square) energy of an audio file and convert this into decibels."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OoSNdq0BHZP_"
   },
   "source": [
    "filename = librosa.ex('nutcracker')\n",
    "x, sr = librosa.load(filename, duration = 20, offset = 2)\n",
    "ipd.display(ipd.Audio(data = x, rate = sr))\n",
    "\n",
    "rms = librosa.feature.rms(y = x)                   # Extract dynamics (RMS)\n",
    "feat = librosa.amplitude_to_db(rms, ref = np.max)  # Convert into dB. Note that this is a relative measure (loudest is now 0)\n",
    "#feat = librosa.feature.spectral_centroid(x)       # Calculate spectral centroid\n",
    "times = librosa.times_like(feat)                   # Create X-axis for time in seconds\n",
    "\n",
    "plt.figure(figsize=(12, 4))                        # Create the plot placeholder\n",
    "plt.plot(times, feat[0], color =  'black')         # Plot feature across time\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### _Learning Task 2_\n",
    "---\n",
    "_There is another feature called _spectral centroid_ that is often used as a proxy for perceptual brightness that you could visualise across time. Calculation of this is very simple and shown above (commented out). You could explore it or some other features from [spectral features](https://librosa.org/doc/latest/feature.html). Note that the labelling of the Y axis needs to be fixed._"
   ],
   "metadata": {
    "id": "lQ06Nkwv272c"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kql17pRbHbvd"
   },
   "source": [
    "# Section 4: Analyse pitch\n",
    "This is best done with a monophonic example, although `librosa` has algorithm to track all pitches as well. Here we extract the fundamental frequency of the trumpet solo using a probabilistic variant of the so-called YIN method. The variant is by Mauch and Dixon (2014) and the original technique was proposed by De Cheveigne and Kawahara (2002)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KJ7ukgJrHd4p"
   },
   "source": [
    "filename = librosa.ex('trumpet')              # Mihai Sorohan - Monophonic trumpet recording\n",
    "x, sr = librosa.load(filename)\n",
    "ipd.display(ipd.Audio(data = x, rate = sr))   # code added to create playback object\n",
    "\n",
    "# This is where the YIN algorithm is applied. fmin refers to the lower frequency threshold and fmax to higher frequence threshold.\n",
    "f0, voiced_flag, voiced_probs = librosa.pyin(x, fmin = librosa.note_to_hz('C2'), fmax = librosa.note_to_hz('C6'))\n",
    "times = librosa.times_like(f0)\n",
    "\n",
    "# Plot the fundamental frequency, make the y-axis logarithmic for easier interpretation\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(times, f0, 'ro:', linewidth = 2)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Hz')\n",
    "ax=plt.gca()\n",
    "ax.yaxis.set_major_formatter(librosa.display.LogHzFormatter())\n",
    "ax.set(yticks=[320,320+80,320+80*2,320+80*3,320+80*4,320+80*5])\n",
    "ax.set(ylim=[300, 800])\n",
    "ax.yaxis.set_major_formatter(librosa.display.LogHzFormatter())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGxCiVtTHfWc"
   },
   "source": [
    "# Section 5: Analyse spectrum\n",
    "\n",
    "Spectrum is a useful representation of audio that shows the energy across frequencies. Try out plotting the spectrogram with the trumpet sound.\n",
    "\n",
    "## Section 5.1: Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1lazjn7vHjJ9"
   },
   "source": [
    "filename = librosa.ex('trumpet') #PROMPT: Try this out with the trumpet audio example. What is the spectrogram telling you?\n",
    "x, sr = librosa.load(filename)  #for the example1d file, put a # in front of this line of code\n",
    "\n",
    "#url = \"https://raw.githubusercontent.com/annaliesemg/audio/master/example1d.wav\" #PROMPT: Try out this other example. Which frequencies are the most prominent (lower ones or higher ones)?\n",
    "#x, sr = sf.read(io.BytesIO(urlopen(url).read()))\n",
    "\n",
    "stft = np.abs(librosa.stft(x))\n",
    "freqs = librosa.fft_frequencies(sr = sr)\n",
    "\n",
    "ipd.display(ipd.Audio(data = x, rate = sr))\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(stft, ref = np.max), x_axis = 'time', y_axis = 'log')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 5.2 Spectrum\n",
    "Here we take only the last note of the trumpet example. Which frequency has the highest amplitude? First we extract the fundamental frequency and print the results out."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1JY4Mgq1B1AT"
   },
   "source": [
    "filename = librosa.ex('trumpet')                               # Mihai Sorohan - Monophonic trumpet recording\n",
    "x, sr = librosa.load(filename, offset = 2.6, duration = 0.75)  # Take the last note (from 2.6 to 3.35 s)\n",
    "### OR USE ANOTHER AUDIO FILE BY UNCOMMENTING THE NEXT 2 LINES  ####\n",
    "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/clar_one_note.wav\"\n",
    "#x, sr = sf.read(io.BytesIO(urlopen(url).read()))\n",
    "\n",
    "ipd.display(ipd.Audio(data=x, rate=sr))                        # Create the playback option\n",
    "stft = np.abs(librosa.stft(x))                                 # Short-term Fourier transform to get the frequencies\n",
    "# Estimate fundamental frequency using an algorithm (pyin)\n",
    "f0, voiced_flag, voiced_probs = librosa.pyin(x, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
    "f=np.nanmedian(f0)                                             # Get the Hz of the fundamental frequency for nice labels\n",
    "n=librosa.hz_to_note(f)                                        # Convert Hz to note name\n",
    "print('Fundamental is', round(f,1),'Hz which is', n )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now display the spectrum of this note."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "# collapse across time and plot a spectrum representation (energy across frequencies)\n",
    "Dmean = stft.mean(axis=1)/max(stft.mean(axis=1))\n",
    "plt.plot(freqs, Dmean, color = 'm')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.xlim([300, 2000])             # Constrain X-axis (Hz)\n",
    "x=np.arange(f,f*10,f)             # Put labels to the multiples of the fundamental\n",
    "plt.xticks(x)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omUNIRYyHhwQ"
   },
   "source": [
    "---\n",
    "### _Learning Task 3_\n",
    "---\n",
    "_What does the spectrum of **clarinet** or **harpsichord** or **harp** look like? What tones are they playing? Tip: you can just alter the beginning of the 4.2 and uncomment the section that loads files from internet._"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Section 6: Analyse onsets and tempo\n",
    "This section estimates the strength of **possible onsets** and then detects the onsets that are stronger than a threshold defined in the algorithm. There is also a beat tracking that uses the onsets to estimate the plausible steady beats in the audio. This information is then used to infer tempo (in the next section).\n",
    "\n",
    "## Section 6.1 Onsets and steady beat tracking\n",
    "Look at the \"Choice\" example at first. Take the first 20 seconds (as duration) of this file. Afterwards, have a look at the 'brahms' example as well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oolz7CufHnOF"
   },
   "source": [
    "filename = librosa.ex('choice')                                             # Try this example first\n",
    "#filename = librosa.ex('brahms')                                            # Hungarian Dance no. 5\n",
    "#filename = librosa.ex('nutcracker')                                        # Nutcracker\n",
    "x, sr = librosa.load(filename, duration = 10)\n",
    "\n",
    "o_env = librosa.onset.onset_strength(x, sr = sr,aggregate = np.median)      # Strength of onsets\n",
    "times = librosa.times_like(o_env, sr = sr)                                  # Create timeline\n",
    "onset_frames = librosa.onset.onset_detect(onset_envelope = o_env, sr = sr)  # Detect onsets\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(y = x,sr = sr, color = 'green')\n",
    "plt.xlim([0,8])\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(times, o_env, label = 'Onset strength')\n",
    "plt.vlines(times[onset_frames], 0, o_env.max(), color = 'r', alpha = 0.9,\n",
    "           linestyle='--', label='Onsets')\n",
    "plt.legend()\n",
    "\n",
    "# Sonify the detected beat events\n",
    "tempo, beats = librosa.beat.beat_track(y=x, sr=sr,trim=True,onset_envelope=o_env,tightness=150)\n",
    "y_beats = librosa.clicks(frames=beats, sr=sr)\n",
    "\n",
    "combined = (x[0:len(y_beats)]+y_beats)/2\n",
    "librosa.display.waveplot(y=abs(y_beats)*max(o_env),sr=sr,color='y')\n",
    "plt.ylim([0,20])\n",
    "plt.xlim([0,8])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "ipd.display(ipd.Audio(data=combined, rate=sr))"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'librosa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/b0/vtr2rd_96119zlr64t5hvlgr0000gp/T/ipykernel_21308/3407150792.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mfilename\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlibrosa\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'choice'\u001B[0m\u001B[0;34m)\u001B[0m                                             \u001B[0;31m# Try this example first\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;31m#filename = librosa.ex('brahms')                                            # Hungarian Dance no. 5\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m#filename = librosa.ex('nutcracker')                                        # Nutcracker\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlibrosa\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mduration\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'librosa' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### _Learning Task 4_\n",
    "---\n",
    "_What are we seeing here? Can you explain the difference between onset strength (blue), detected onsets (red dotted lines) and beats (yellow)? Explore how these features behave in other examples (TIP: beginning of the code in Section 5.1 has the other examples commented out). How well are the onset detected and is the beat structure always what you think it should be?_\n"
   ],
   "metadata": {
    "id": "uP3079Yn5WBU"
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 6.2 Analyse tempo\n",
    "\n",
    "---\n",
    "### _Learning Task 5_\n",
    "---\n",
    "\n",
    "_Find **tempo** of the file that you analysed above. The tempo is indicated in Beats Per Minute (BPM).\n",
    "*Tip. There is a command called* `beat.tempo` *that can be used to calculate the tempo.* The following script will not work until you have completed the function in the first line._"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j7zSbYC7NgNS"
   },
   "source": [
    "tempo = librosa.b...(x, aggregate = np.median)\n",
    "print(np.round(tempo,1),'BPM')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
