{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuomaseerola/music_and_science_seminar/blob/master/corpus_analysis_tutorial_key.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTO_mMkGJCci"
      },
      "source": [
        "# Music and Science – Audio Corpus Analysis Tutorial \n",
        "\n",
        "[Tuomas Eerola](https://www.durham.ac.uk/staff/tuomas-eerola/), Durham University, Music Department, 2023.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3LSkxYNS5nY"
      },
      "source": [
        "#PROMPT: Press the play button to set up the technical system (import libraries etc.)\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from matplotlib import pyplot as plt \n",
        "%matplotlib inline\n",
        "print(librosa.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2SD2rhaHDF9"
      },
      "source": [
        "# 1. Get an audio corpus using mirdata\n",
        "\n",
        "`mirdata` has numerous corpora with annotations so we get the dance music beatport dataset for this purpose.\n",
        "\n",
        "\n",
        "The Beatport EDM Key Dataset includes 1486 two-minute sound excerpts from various EDM subgenres, annotated with single-key labels, comments and confidence levels generously provided by Eduard Mas Marín, nd thoroughly revised and expanded by Ángel Faraldo (2017).\n",
        "    The original audio samples belong to online audio snippets from Beatport, an online music store for DJ's and Electronic Dance Music Producers ([http:\\\\www.beatport.com](http:\\\\www.beatport.com)). If this dataset were used in further research,we would appreciate the citation of the current DOI ([10.5281/zenodo.1101082](10.5281/zenodo.1101082)).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trn2ya5RNovU"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install mirdata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mirdata\n",
        "beatport_key = mirdata.initialize('beatport_key')\n",
        "beatport_key.download()"
      ],
      "metadata": {
        "id": "WdKM48pz-FOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next command defines key-finding function `Tonal_Fragment` that we will be using later."
      ],
      "metadata": {
        "id": "mA6jSnm6TzAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "# class that uses the librosa library to analyze the key that an mp3 is in\n",
        "# arguments:\n",
        "#     waveform: an mp3 file loaded by librosa, ideally separated out from any percussive sources\n",
        "#     sr: sampling rate of the mp3, which can be obtained when the file is read with librosa\n",
        "#     tstart and tend: the range in seconds of the file to be analyzed; default to the beginning and end of file if not specified\n",
        "class Tonal_Fragment(object):\n",
        "    def __init__(self, waveform, sr, tstart=None, tend=None):\n",
        "        self.waveform = waveform\n",
        "        self.sr = sr\n",
        "        self.tstart = tstart\n",
        "        self.tend = tend\n",
        "        \n",
        "        if self.tstart is not None:\n",
        "            self.tstart = librosa.time_to_samples(self.tstart, sr=self.sr)\n",
        "        if self.tend is not None:\n",
        "            self.tend = librosa.time_to_samples(self.tend, sr=self.sr)\n",
        "        self.y_segment = self.waveform[self.tstart:self.tend]\n",
        "        self.chromograph = librosa.feature.chroma_cqt(y=self.y_segment, sr=self.sr, n_octaves=5, threshold=0.07, fmin=65.4, bins_per_octave=36,hop_length=8192)\n",
        "        \n",
        "        # chroma_vals is the amount of each pitch class present in this time interval\n",
        "        self.chroma_vals = []\n",
        "        for i in range(12):\n",
        "            self.chroma_vals.append(np.sum(self.chromograph[i]))\n",
        "        pitches = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B']\n",
        "        # dictionary relating pitch names to the associated intensity in the song\n",
        "        self.keyfreqs = {pitches[i]: self.chroma_vals[i] for i in range(12)} \n",
        "        \n",
        "        keys = [pitches[i] + ' major' for i in range(12)] + [pitches[i] + ' minor' for i in range(12)]\n",
        "\n",
        "        # use of the Krumhansl-Schmuckler key-finding algorithm, which compares the chroma\n",
        "        # data above to typical profiles of major and minor keys:\n",
        "        maj_profile = [6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88]\n",
        "        min_profile = [6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17]\n",
        "\n",
        "        # Temperley profiles\n",
        "        maj_profile = [0.748, 0.060, 0.488, 0.082, 0.670, 0.460, 0.096, 0.715, 0.104, 0.366, 0.057, 0.400]\n",
        "        min_profile = [0.712, 0.084, 0.474, 0.618, 0.049, 0.460, 0.105, 0.747, 0.404, 0.067, 0.133, 0.330]\n",
        "\n",
        "        # finds correlations between the amount of each pitch class in the time interval and the above profiles,\n",
        "        # starting on each of the 12 pitches. then creates dict of the musical keys (major/minor) to the correlation\n",
        "        self.min_key_corrs = []\n",
        "        self.maj_key_corrs = []\n",
        "        for i in range(12):\n",
        "            key_test = [self.keyfreqs.get(pitches[(i + m)%12]) for m in range(12)]\n",
        "            # correlation coefficients (strengths of correlation for each key)\n",
        "            self.maj_key_corrs.append(round(np.corrcoef(maj_profile, key_test)[1,0], 3)) # cosine distance is better\n",
        "            self.min_key_corrs.append(round(np.corrcoef(min_profile, key_test)[1,0], 3))\n",
        "\n",
        "        # names of all major and minor keys\n",
        "        self.key_dict = {**{keys[i]: self.maj_key_corrs[i] for i in range(12)}, \n",
        "                         **{keys[i+12]: self.min_key_corrs[i] for i in range(12)}}\n",
        "        \n",
        "        # this attribute represents the key determined by the algorithm\n",
        "        self.key = max(self.key_dict, key=self.key_dict.get)\n",
        "        self.bestcorr = max(self.key_dict.values())\n",
        "        \n",
        "        # this attribute represents the second-best key determined by the algorithm,\n",
        "        # if the correlation is close to that of the actual key determined\n",
        "        self.altkey = None\n",
        "        self.altbestcorr = None\n",
        "\n",
        "        for key, corr in self.key_dict.items():\n",
        "            if corr > self.bestcorr*0.9 and corr != self.bestcorr:\n",
        "                self.altkey = key\n",
        "                self.altbestcorr = corr\n",
        "                \n",
        "    # prints the relative prominence of each pitch class            \n",
        "    def print_chroma(self):\n",
        "        self.chroma_max = max(self.chroma_vals)\n",
        "        for key, chrom in self.keyfreqs.items():\n",
        "            print(key, '\\t', f'{chrom/self.chroma_max:5.3f}')\n",
        "\n",
        "    # plots the relative prominence of each pitch class            \n",
        "    def plot_chroma(self, title=None):\n",
        "        self.chroma_max = max(self.chroma_vals)\n",
        "        pc = []\n",
        "        pckey = []\n",
        "        for key, chrom in self.keyfreqs.items():\n",
        "            pckey.append(key)\n",
        "            pc.append(chrom/self.chroma_max)\n",
        "        print('print')    \n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.bar(pckey,pc,color='darkred')\n",
        "        plt.xlabel(\"Pitch classes\")\n",
        "        plt.ylabel(\"Relative weight\")\n",
        "        if title is None:\n",
        "            plt.title('Pitch Class Distribution')\n",
        "        else:\n",
        "            plt.title(title)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "                \n",
        "    # prints the correlation coefficients associated with each major/minor key\n",
        "    def corr_table(self):\n",
        "        for key, corr in self.key_dict.items():\n",
        "            print(key, '\\t', f'{corr:6.3f}')\n",
        "    \n",
        "    # printout of the key determined by the algorithm; if another key is close, that key is mentioned\n",
        "    def print_key(self):\n",
        "        print(\"likely key: \", max(self.key_dict, key=self.key_dict.get), \", correlation: \", self.bestcorr, sep='')\n",
        "        if self.altkey is not None:\n",
        "                print(\"also possible: \", self.altkey, \", correlation: \", self.altbestcorr, sep='')\n",
        "    \n",
        "    # prints a chromagram of the file, showing the intensity of each pitch class over time\n",
        "    def chromagram(self, title=None):\n",
        "        C = librosa.feature.chroma_cqt(y=self.waveform, sr=sr, bins_per_octave=24)\n",
        "        plt.figure(figsize=(12,4))\n",
        "        librosa.display.specshow(C, sr=sr, x_axis='time', y_axis='chroma', vmin=0, vmax=1)\n",
        "        if title is None:\n",
        "            plt.title('Chromagram')\n",
        "        else:\n",
        "            plt.title(title)\n",
        "        plt.colorbar()\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "56OKrNG4-qPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example track"
      ],
      "metadata": {
        "id": "IOzr-75h-zUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ID = 25\n",
        "beatport_key_ids = beatport_key.track_ids\n",
        "beatport_key_data = beatport_key.load_tracks()\n",
        "example_track = beatport_key_data[beatport_key_ids[ID]]\n",
        "print(example_track.track_id, example_track.title, example_track.key, example_track.tempo)"
      ],
      "metadata": {
        "id": "Uwj3etyo-yhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from librosa.core import audio\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.figure(figsize=(12, 3))\n",
        "y,sr = librosa.load(example_track.audio_path,offset=5,duration=30)\n",
        "#y = example_track.audio[0]\n",
        "\n",
        "sr = example_track.audio[1]\n",
        "librosa.display.waveshow(y=y,sr=sr)\n",
        "ipd.display(ipd.Audio(data=y, rate=sr))"
      ],
      "metadata": {
        "id": "eE_zIRdB-4sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This track is annotated to be in G minor key (and alternatively in Bb major, but we only look at the first key). Let's try the algorithm. We apply our custom function, Tonal_Fragment, which extracts the chromagram, summarise the energies across the pitch-classes, and correlates these to the major and minor key-profiles, and infers the key. Let's look at the elements in turn."
      ],
      "metadata": {
        "id": "pTUq2oQJ--QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ton = Tonal_Fragment(y, sr)                           # key-finding\n",
        "ton.chromagram(example_track.title)                   # plot chromagram"
      ],
      "metadata": {
        "id": "3EXiMon4-5_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ton.plot_chroma(example_track.title)"
      ],
      "metadata": {
        "id": "J9PRycZo_Gyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ton.print_key()\n",
        "ton.corr_table()\n"
      ],
      "metadata": {
        "id": "wY7kbPUe_IvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's see how this works on a corpus"
      ],
      "metadata": {
        "id": "AG4w3Pdy_KeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install mir_eval\n",
        "import mir_eval"
      ],
      "metadata": {
        "id": "Dy5nG0zk_OA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i in tqdm(range(len(beatport_key_data))):\n",
        "#for i in tqdm(range(604,1000)):\n",
        "    example_track = beatport_key_data[beatport_key_ids[i]]\n",
        "    y,sr = librosa.load(example_track.audio_path,offset=15,duration=2)\n",
        "    ton = Tonal_Fragment(y, sr)\n",
        "    estimated_key = max(ton.key_dict, key=ton.key_dict.get)\n",
        "    estimated_keycor = ton.bestcorr\n",
        "    score = []\n",
        "    r_key=example_track.key[0]\n",
        "    #print([estimated_key + ' -> ' + r_key])\n",
        "    try:\n",
        "        mir_eval.key.validate_key(r_key)\n",
        "    except ValueError as ve:\n",
        "        print(f' {r_key} is not a valid key.')\n",
        "        results.append(-1)\n",
        "    if r_key=='X':\n",
        "        print(f' {r_key} is not a valid key.')\n",
        "        results.append(-1)\n",
        "    elif r_key=='x':\n",
        "        print(f' {r_key} is not a valid key.')\n",
        "        results.append(-1)\n",
        "    else:\n",
        "        reference_key = r_key.split(' ')[0] + ' ' + r_key.split(' ')[1]\n",
        "        if not '^' in reference_key and not '_' in reference_key:\n",
        "            score.append(mir_eval.key.weighted_score(reference_key, estimated_key))\n",
        "        if not '^' in reference_key and not '_' in reference_key:\n",
        "            results.append(str(max(score)))\n"
      ],
      "metadata": {
        "id": "oapLS6Su_Q4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile results"
      ],
      "metadata": {
        "id": "rUz5Z7lI_5RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(results)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u5Kyp9TSAfbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt \n",
        "import numpy as np  \n",
        "\n",
        "unique, counts = np.unique(results, return_counts=True)\n",
        "print(unique) # ['-1' '0.0' '0.2' '0.3' '0.5' '1.0']\n",
        "print(counts) # [123 541 183  66  52 521]\n",
        "\n",
        "print(round(sum(counts[3:6])/sum(counts[1:6]),2)) #  45% partially correct\n",
        "print(round(sum(counts[5:6])/sum(counts[1:6]),2)) #  32% fully correct\n"
      ],
      "metadata": {
        "id": "9BL-mNQ-HoWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "* Krumhansl, C. L. (1990). _Cognitive foundations of musical pitch_. Oxford, UK: Oxford University Press.\n",
        "* Faraldo, Á. (2017). Tonality Estimation in Electronic Dance Music: A Computational and Musically Informed Examination. PhD Thesis. Universitat Pompeu Fabra, Barcelona."
      ],
      "metadata": {
        "id": "JtKivqcWHrSA"
      }
    }
  ]
}